<!DOCTYPE html>
<html lang="en">
<head>
	<meta name="google-site-verification" content="S5nCw19pRq7mIILllS376ov6_nr0qm_EJrEyhYHOdWM" />
	<meta charset="utf-8">
	<title>Time-of-Day Neural Style Transfer for Architectural Photographs</title>
	<meta content="width=device-width, initial-scale=1.0" name="viewport">
	<meta content="time of day, neural style transfer, architectual photo" name="keywords">
	<meta content="DayOfTime Project Webpage" name="description">

	<!-- Favicon -->
	<link href="img/logo.svg" rel="icon">

	<!-- Google Web Fonts -->
	<link rel="preconnect" href="https://fonts.gstatic.com">
	<link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700&display=swap" rel="stylesheet">

	<!-- Font Awesome -->
	<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.10.0/css/all.min.css" rel="stylesheet">

	<!-- Customized Bootstrap Stylesheet -->
	<link href="css/style.css" rel="stylesheet">

	<!-- Google Analytics tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-B9LP733HN2"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-B9LP733HN2');
	</script>
</head>


<body>
	<header class="subsection header-style">
		<h1>Time-of-Day Neural Style Transfer <br>for Architectural Photographs</h1>
		<p><div><a href="http://yschen.site/" target="_blank">Yingshu Chen</a><sup>1</sup>  <a href="https://tuananh1007.github.io/" target="_blank">Tuan-Anh Vu</a><sup>1</sup>   Ka-Chun Shum<sup>1</sup> 
		<a href="https://sonhua.github.io/" target="_blank">Binh-Son Hua</a><sup>2</sup> <a href="https://www.saikit.org/" target="_blank">Sai-Kit Yeung</a><sup>1</sup>
		<br><sup>1</sup>The Hong Kong University of Science and Technology    <sup>2</sup>VinAI Research</div></p>
		<p class="text-black">International Conference on Computational Photography (ICCP) 2022 (Oral)</p>
		<div class="btn-group-light">
			<a href="assets/ICCP_2022_paper.pdf" target="_blank"><button class="btn-light btn-header">Paper</button></a>
			<a href="assets/ICCP_2022_supp_doc.pdf" target="_blank"><button class="btn-light btn-header">Supp</button></a>
			<a href="./interactive_viewer"><button class="btn-light btn-header">Supp-Viewer</button></a>
			<a href="https://github.com/hkust-vgd/architectural_style_transfer#dataset" target="_blank"><button class="btn-light btn-header">Data</button></a>
			<a href="https://github.com/hkust-vgd/architectural_style_transfer" target="_blank"><button class="btn-light btn-header">Github</button></a>
		</div>
	</header>

	<!-- Abastract -->
	<section class="subsection">
		<div class="figure-group col row-cols-md-2">
			<img class="figure-img" src="img/teaser.png">
			<img class="figure-img" src="img/teaser2.png">
		</div>
		<div class="figure-caption">
			<strong class="text-dark"> Architectural Photography Style Transfer </strong> aims to transfer background dynamic texture and chrominance, and transfer sufficient styles for foreground while keeping foreground geometry intact.
		</div>
	</section>
	<section class="subsection" id="abstract">
		<div>
			<h2 class="text-dark card-title">Abstract</h2>
			<div class="abstract-text">
				Architectural photography is a genre of photography that focuses on capturing a building or structure in the foreground with dramatic lighting in the background. Inspired by recent successes in image-to-image translation methods, we aim to perform style transfer for architectural photographs. However, the special composition in architectural photography poses great challenges for style transfer in this type of photographs. Existing neural style transfer methods treat the architectural images as a single entity, which would generate mismatched chrominance and destroy geometric features of the original architecture, yielding unrealistic lighting, wrong color rendition, and visual artifacts such as ghosting, appearance distortion, or color mismatching. In this paper, we specialize a neural style transfer method for architectural photography. Our method addresses the composition of the foreground and background in an architectural photograph in a two-branch neural network that separately considers the style transfer of the foreground and the background, respectively. Our method comprises a segmentation module, a learning-based image-to-image translation module, and an image blending optimization module. We trained our image-to-image translation neural network with a new dataset of unconstrained outdoor architectural photographs captured at different magic times of a day, utilizing additional semantic information for better chrominance matching and geometry preservation. Our experiments show that our method can produce photorealistic lighting and color rendition on both the foreground and background, and outperforms general image-to-image translation and arbitrary style transfer baselines quantitatively and qualitatively.
			</div>
		</div>
	</section>


	<!-- Method -->
	<section class="subsection" id="framework">
		<div>
			<h2 class="text-dark card-title">Framework</h2>
			<div class="figure-group col">
				<img class="col-xl-10 figure-img" src="img/framework.png">
			</div>
			<div class="figure-caption text-center">
				Our architectural style transfer framework has three main modules: segmentation, image translation and blending
				optimization.
			</div>
		</div>
	</section>

	<!-- Qualitative Results-->
	<section class="subsection" id="results">
		<h2 class="text-dark card-title">Qualitative Results</h2>
		<div class="col row-cols-1">
			<img class="figure-img" src="img/result_i2i.png">
			<p class="figure-caption">Comparisons among <strong>image-to-image translation</strong> baselines and our proposed method. Our results have plausible colors
				from foreground and background, and preserve the geometry in different style transfer cases.</p>
			<p></p>
			<img class="figure-img" src="img/result_nst.png">
			<p class="figure-caption">Comparisons among <strong>neural style transfer</strong> baselines and proposed method. While neural style transfer methods tend
				to have visual artifacts, our results have matched colors from foreground and background respectively, and preserve the
				geometry of the foreground while generating diverse cloud textures in the background.</p>
		</div>
		<p>For detailed comparison, please refer to supplmentary <a href="./interactive_viewer" target="_blank">interactive viewer</a> page.</p>
	</section>
	<!-- Materials -->
	<section class="subsection" id="materials">
		<h2 class="text-dark card-title">Materials</h2>
		<div class="figure-group row-cols-lg-6 align-items-center justify-content-around">
			<a href="assets/ICCP_2022_paper.pdf"  class="figure-group-vertical" target="_blank">
				<img class="figure-img img-thumbnail" src="img/paper_thumbnail.jpg">
				<div class="figure-caption text-uppercase text-center text-black">Paper</div>
			</a>
			<a href="assets/ICCP_2022_supp_doc.pdf" class="figure-group-vertical" target="_blank">
				<img class="figure-img img-thumbnail" src="img/sub_doc_thumbnail.jpg">
				<div class="figure-caption text-uppercase text-center text-black">Supplementary Doc</div>
			</a>
			<a href="./interactive_viewer" class="figure-group-vertical" target="_blank">
				<img class="figure-img img-thumbnail" src="img/viewer_thumbnail.jpg">
				<div class="figure-caption text-uppercase text-center text-black">Interactive Viewer</div>
			</a>
			<a href="assets/ICCP_2022_poster.pdf" class="figure-group-vertical" target="_blank">
				<img class="figure-img img-thumbnail" src="img/poster_thumbnail.jpg">
				<div class="figure-caption text-uppercase text-center text-black">Poster</div>
			</a>
			<a href="https://github.com/hkust-vgd/architectural_style_transfer#dataset" class="figure-group-vertical" target="_blank">
				<i class="fas fa-database figure-img logo-label "></i>
				<div class="figure-caption text-uppercase text-center text-black">Data</div>
			</a>
			<a href="https://github.com/hkust-vgd/architectural_style_transfer" class="figure-group-vertical text-decoration-none" target="_blank">
				<i class="fab fa-github figure-img logo-label "></i>
				<div class="figure-caption text-uppercase text-center text-black">Code</div>
			</a>
			<a href="assets/ICCP_2022_ppt.pdf" class="figure-group-vertical" target="_blank">
				<i class="fas fa-file-powerpoint figure-img logo-label"></i>
				<div class="figure-caption text-uppercase text-center text-black">Slides</div>
			</a>
			<a href="https://youtu.be/UhfD9YXFRjw?t=3630" class="figure-group-vertical" target="_blank">
				<i class="fab fa-youtube figure-img logo-label"></i>
				<div class="figure-caption text-uppercase text-center text-black">Conference Talk</div>
			</a>


		</div>
	</section>

	<!-- Citation -->
	<section class="subsection" id="citation">
		<h2 class="text-dark card-title">Citation</h2>
		<pre>
			<code>
@inproceedings{chen2022timeofday,
	title={Time-of-Day Neural Style Transfer for Architectural Photographs},
	author={Chen, Yingshu and Vu, Tuan-Anh and Shum, Ka-Chun and Hua, Binh-Son and Yeung, Sai-Kit},
	booktitle={International Conference on Computational Photography (ICCP)},
	year={2022},
	organization={IEEE}
}</code>
		</pre>
	</section>

	<!-- Acknowlegement -->
	<section class="subsection">
		<h2 class="text-dark card-title"> Acknowlegements </h2>
		<p>This paper was partially supported by an internal grant from	HKUST (R9429) and the HKUST-WeBank Joint Lab.</p>
	</section>

	<!-- References -->
	<section class="subsection">
		<h2 class="text-dark card-title">References</h2>
		<p><span class="text-dark">DRIT++</span>&nbsp; <span class="small">H.-Y. Lee, H.-Y. Tseng, J.-B. Huang, M. Singh, and M.-H. Yang, “Diverse image-to-image translation via disentangled representations,” ECCV 2018. <br>
			H.-Y. Lee, H.-Y. Tseng, Q. Mao, J.-B. Huang, Y.-D. Lu, M. Singh, and M.-H. Yang, “DRIT++: Diverse image-to-image translation via disentangled representations,” IJCV 2020.</span>
<!--		</p>-->
		<br><span class="text-dark">MUNIT</span>&nbsp; <span class="small">X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz, “Multimodal
unsupervised image-to-image translation,” ECCV 2018.</span>
		<br><span class="text-dark">FUNIT</span>&nbsp; <span class="small">M.-Y. Liu, X. Huang, A. Mallya, T. Karras, T. Aila, J. Lehtinen, and
J. Kautz, “Few-shot unsupervised image-to-image translation,” ICCV 2019.</span>
		<br><span class="text-dark">DSMAP</span>&nbsp; <span class="small">H.-Y. Chang, Z. Wang, and Y.-Y. Chuang, “Domain-specific mappings
for generative adversarial style transfer,” ECCV 2020.</span>
		<br><span class="text-dark">AdaIN</span>&nbsp; <span class="small">X. Huang and S. Belongie, “Arbitrary style transfer in real-time
with adaptive instance normalization,” ICCV 2017.</span>
		<br><span class="text-dark">SANet</span>&nbsp; <span class="small">D. Y. Park and K. H. Lee, “Arbitrary style transfer with styleattentional
networks,” CVPR 2019.</span>
		<br><span class="text-dark">AdaAttN</span>&nbsp; <span class="small">S. Liu, T. Lin, D. He, F. Li, M. Wang, X. Li, Z. Sun, Q. Li, and
E. Ding, “AdaAttN: Revisit attention mechanism in arbitrary neural
style transfer,” ICCV 2021.</span>
		<br><span class="text-dark">LST</span>&nbsp; <span class="small">X. Li, S. Liu, J. Kautz, and M.-H. Yang, “Learning linear transformations
for fast arbitrary style transfer,” CVPR 2019.</span></br>
<!--		<p><span class="text-dark"></span>&nbsp; <span class="small"></span></p>-->
	</section>

	<section class="subsection footer">
		<hr>
        <p class="text-center">Webpage is maintained by yingshu2008[AT]gmail[DOT]com</p>
    </section>

</body>
